{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for NLP\n",
    "\n",
    "There are lots of feature transformations that need to be done on text data to get it to a point that machine learning algorithms can understand. Luckily, Spark has placed the most important ones in convienent Feature Transformer calls. \n",
    "\n",
    "Let's go over them before jumping into the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/gkouskosv/spark-2.4.5-bin-hadoop2.6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp_tools').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "<p><a href=\"http://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\">Tokenization</a> is the process of taking text (such as a sentence) and breaking it into individual terms (usually words).  A simple <a href=\"api/scala/index.html#org.apache.spark.ml.feature.Tokenizer\">Tokenizer</a> class provides this functionality.  The example below shows how to split sentences into sequences of words.</p>\n",
    "\n",
    "<p><a href=\"api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer\">RegexTokenizer</a> allows more\n",
    " advanced tokenization based on regular expression (regex) matching.\n",
    " By default, the parameter &#8220;pattern&#8221; (regex, default: <code>\"\\\\s+\"</code>) is used as delimiters to split the input text.\n",
    " Alternatively, users can set parameter &#8220;gaps&#8221; to false indicating the regex &#8220;pattern&#8221; denotes\n",
    " &#8220;tokens&#8221; rather than splitting gaps, and find all matching occurrences as the tokenization result.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDF = spark.createDataFrame([\n",
    "    (0, 'Hi I heard about Spark'),\n",
    "    (1, 'I wish Java could use case classes'),\n",
    "    (2, 'Logistic,regression,models,are,neat')], ['id', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol='sentence', outputCol='words',\\\n",
    "                                pattern='\\\\W')\n",
    "# alternatively, pattern='\\\\w', gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), returnType=IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDF)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.select(['sentence','words'])\\\n",
    "        .withColumn('tokens', countTokens(col('words'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized.select(['sentence','words']).withColumn('tokens', countTokens(col('words')))\\\n",
    "            .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stop Words Removal\n",
    "\n",
    "<p><a href=\"https://en.wikipedia.org/wiki/Stop_words\">Stop words</a> are words which\n",
    "should be excluded from the input, typically because the words appear\n",
    "frequently and don&#8217;t carry as much meaning.</p>\n",
    "\n",
    "<p><code>StopWordsRemover</code> takes as input a sequence of strings (e.g. the output\n",
    "of a <a href=\"ml-features.html#tokenizer\">Tokenizer</a>) and drops all the stop\n",
    "words from the input sequences. The list of stopwords is specified by\n",
    "the <code>stopWords</code> parameter. Default stop words for some languages are accessible \n",
    "by calling <code>StopWordsRemover.loadDefaultStopWords(language)</code>, for which available \n",
    "options are &#8220;danish&#8221;, &#8220;dutch&#8221;, &#8220;english&#8221;, &#8220;finnish&#8221;, &#8220;french&#8221;, &#8220;german&#8221;, &#8220;hungarian&#8221;, \n",
    "&#8220;italian&#8221;, &#8220;norwegian&#8221;, &#8220;portuguese&#8221;, &#8220;russian&#8221;, &#8220;spanish&#8221;, &#8220;swedish&#8221; and &#8220;turkish&#8221;. \n",
    "A boolean parameter <code>caseSensitive</code> indicates if the matches should be case sensitive \n",
    "(false by default).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0, 'I saw the red ballon'.split()),\n",
    "    (1, 'Mary had a little lamb'.split())\n",
    "], ['id', 'raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                 raw|\n",
      "+---+--------------------+\n",
      "|  0|[I, saw, the, red...|\n",
      "|  1|[Mary, had, a, li...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, ballon]  |[saw, red, ballon]  |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol='raw', outputCol='filtered')\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "An n-gram is a sequence of nn tokens (typically words) for some integer nn. The NGram class can be used to transform input features into nn-grams.\n",
    "\n",
    "<p><code>NGram</code> takes as input a sequence of strings (e.g. the output of a <a href=\"ml-features.html#tokenizer\">Tokenizer</a>).  The parameter <code>n</code> is used to determine the number of terms in each $n$-gram. The output will consist of a sequence of $n$-grams where each $n$-gram is represented by a space-delimited string of $n$ consecutive words.  If the input sequence contains fewer than <code>n</code> strings, no output is produced.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
